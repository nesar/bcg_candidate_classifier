#!/usr/bin/env python3
"""
Diagnostic Plotting for BCG Candidate Classifier

This module creates comprehensive diagnostic plots from evaluation_results.csv files
generated by the BCG candidate classifier testing pipeline.

Features:
- Accuracy and error distribution analysis
- Distance error statistics and distributions  
- Redshift dependence of biases
- Uncertainty quantification analysis (if available)
- Detection performance metrics
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

# Set style for publication-quality plots consistent with plot_physical_results.py
plt.rcParams.update({"text.usetex":False,"font.family":"serif","mathtext.fontset":"cm","axes.linewidth":1.2})
plt.style.use('default')
sns.set_palette("husl")


def load_evaluation_results(results_file):
    """Load and validate evaluation results CSV file."""
    if not os.path.exists(results_file):
        raise FileNotFoundError(f"Results file not found: {results_file}")
    
    df = pd.read_csv(results_file)
    
    # Validate required columns
    required_cols = ['pred_x', 'pred_y', 'true_x', 'true_y', 'distance_error']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        raise ValueError(f"Missing required columns: {missing_cols}")
    
    print(f"Loaded {len(df)} evaluation results")
    print(f"Available columns: {', '.join(df.columns)}")
    
    return df


def create_diagnostic_plots(results_file, output_dir=None, figsize=(16, 12)):
    """
    Create comprehensive diagnostic plots from evaluation results.
    
    Args:
        results_file: Path to evaluation_results.csv
        output_dir: Directory to save plots (defaults to same dir as results)
        figsize: Figure size for the subplot grid
    """
    
    # Load data
    df = load_evaluation_results(results_file)
    
    # Set output directory
    if output_dir is None:
        output_dir = os.path.dirname(results_file)
    
    # Create figure with subplots
    fig, axes = plt.subplots(2, 3, figsize=figsize)
    fig.suptitle('BCG Candidate Classifier - Diagnostic Analysis', fontsize=18, fontweight='bold')
    
    # Define distance threshold for "correct" detections (in pixels)
    distance_threshold = 10.0  # pixels
    
    # Calculate accuracy metrics
    correct_detections = df['distance_error'] <= distance_threshold
    accuracy = np.mean(correct_detections) * 100
    
    # Plot 1: Rank-based Detection Success Analysis (Single Target)
    ax1 = axes[0, 0]

    # Use rank-based analysis if available
    if 'bcg_rank' in df.columns and not df['bcg_rank'].isna().all():
        # Rank-based success analysis
        ranks = df['bcg_rank'].dropna()

        # Count successes by rank
        rank_1_count = len(ranks[ranks == 1])
        rank_2_count = len(ranks[ranks == 2])
        rank_3_count = len(ranks[ranks == 3])
        rank_other_count = len(ranks[ranks > 3])
        no_success_count = len(df) - len(ranks)  # Cases where true BCG not found in any rank

        counts = [rank_1_count, rank_2_count, rank_3_count, rank_other_count, no_success_count]
        labels = [
            f'Best Prediction\n(Rank 1)\n{rank_1_count} ({rank_1_count/len(df)*100:.1f}%)',
            f'2nd Best Prediction\n(Rank 2)\n{rank_2_count} ({rank_2_count/len(df)*100:.1f}%)',
            f'3rd Best Prediction\n(Rank 3)\n{rank_3_count} ({rank_3_count/len(df)*100:.1f}%)',
            f'Lower Rank\n(Rank >3)\n{rank_other_count} ({rank_other_count/len(df)*100:.1f}%)',
            f'Not Detected\n{no_success_count} ({no_success_count/len(df)*100:.1f}%)'
        ]
        colors = ['#2ecc71', '#f39c12', '#e67e22', '#9b59b6', '#e74c3c']

        # Filter out zero counts for cleaner display
        non_zero_indices = [i for i, count in enumerate(counts) if count > 0]
        counts = [counts[i] for i in non_zero_indices]
        labels = [labels[i] for i in non_zero_indices]
        colors = [colors[i] for i in non_zero_indices]

        wedges, texts, autotexts = ax1.pie(counts, labels=labels, colors=colors, autopct='',
                                           startangle=90, textprops={'fontsize': 9})

        # Calculate top-3 success rate
        top3_success = (rank_1_count + rank_2_count + rank_3_count) / len(df) * 100
        ax1.set_title(f'Single-Target Rank Analysis\nTop-3 Success: {top3_success:.1f}%', fontweight='bold', fontsize=16)
    else:
        # Fall back to traditional distance-based analysis
        counts = [np.sum(correct_detections), np.sum(~correct_detections)]
        labels = [f'Correct\n(≤{distance_threshold} px)\n{counts[0]} ({counts[0]/len(df)*100:.1f}%)',
                  f'Incorrect\n(>{distance_threshold} px)\n{counts[1]} ({counts[1]/len(df)*100:.1f}%)']
        colors = ['#2ecc71', '#e74c3c']

        wedges, texts, autotexts = ax1.pie(counts, labels=labels, colors=colors, autopct='',
                                           startangle=90, textprops={'fontsize': 10})
        ax1.set_title(f'Single-Target Detection\nOverall: {accuracy:.1f}%', fontweight='bold', fontsize=16)
    
    # Plot 2: Distance Error Distribution
    ax2 = axes[0, 1]
    distances = df['distance_error'].dropna()
    
    # Filter out infinite values for plotting
    finite_distances = distances[np.isfinite(distances)]
    n_infinite = len(distances) - len(finite_distances)
    
    if len(finite_distances) > 0:
        # Histogram with KDE for finite distances only
        ax2.hist(finite_distances, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')
        
        # Add KDE if scipy is available
        try:
            from scipy import stats
            if len(finite_distances) > 1:  # Need at least 2 points for KDE
                kde = stats.gaussian_kde(finite_distances)
                x_range = np.linspace(finite_distances.min(), finite_distances.max(), 200)
                ax2.plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')
                ax2.legend()
        except ImportError:
            pass
        
        # Add statistics for finite distances
        median_dist = np.median(finite_distances)
        mean_dist = np.mean(finite_distances)
        ax2.axvline(median_dist, color='red', linestyle='--', alpha=0.8, label=f'Median: {median_dist:.1f}')
        ax2.axvline(mean_dist, color='orange', linestyle='--', alpha=0.8, label=f'Mean: {mean_dist:.1f}')
        ax2.axvline(distance_threshold, color='green', linestyle=':', alpha=0.8, label=f'Threshold: {distance_threshold}')
        
        title_text = f'Distance Error Distribution\n({len(finite_distances)} finite values'
        if n_infinite > 0:
            title_text += f', {n_infinite} failed predictions)'
        else:
            title_text += ')'
            
        ax2.set_title(title_text)
    else:
        ax2.text(0.5, 0.5, f'No finite distance errors found\n({n_infinite} failed predictions)', 
                ha='center', va='center', transform=ax2.transAxes, fontsize=12)
        ax2.set_title('Distance Error Distribution')
    
    ax2.set_xlabel('Distance Error (pixels)', fontsize=18)
    ax2.set_ylabel('Density', fontsize=18)
    ax2.tick_params(axis='both', labelsize=18)
    ax2.legend(fontsize=18)
    ax2.grid(True, alpha=0.3)
    
    # Plot 3: Redshift Dependence (if redshift data available)
    ax3 = axes[0, 2]
    if 'z' in df.columns and not df['z'].isna().all():
        z_data = df.dropna(subset=['z'])
        
        # Scatter plot of distance error vs redshift
        scatter = ax3.scatter(z_data['z'], z_data['distance_error'], 
                            alpha=0.6, s=30, c=z_data['distance_error'], 
                            cmap='viridis', edgecolors='black', linewidth=0.5)
        
        # Add trend line if enough data
        if len(z_data) > 5:
            try:
                z_fit = np.polyfit(z_data['z'], z_data['distance_error'], 1)
                z_trend = np.poly1d(z_fit)
                z_range = np.linspace(z_data['z'].min(), z_data['z'].max(), 100)
                ax3.plot(z_range, z_trend(z_range), 'r--', alpha=0.8, linewidth=2)
                
                # Calculate correlation
                corr = np.corrcoef(z_data['z'], z_data['distance_error'])[0, 1]
                ax3.text(0.05, 0.95, f'Correlation: {corr:.3f}', transform=ax3.transAxes,
                        bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))
            except:
                pass
        
        ax3.set_xlabel('Redshift (z)', fontsize=18)
        ax3.set_ylabel('Distance Error (pixels)', fontsize=18)
        ax3.set_title('Distance Error vs Redshift', fontsize=18)
        ax3.tick_params(axis='both', labelsize=18)
        ax3.grid(True, alpha=0.3)
        
        # Add colorbar
        cbar = plt.colorbar(scatter, ax=ax3)
        cbar.set_label('Distance Error (pixels)', rotation=270, labelpad=15)
    else:
        ax3.text(0.5, 0.5, 'No redshift data\navailable', ha='center', va='center',
                transform=ax3.transAxes, fontsize=12, bbox=dict(boxstyle="round,pad=0.3", 
                facecolor="lightgray", alpha=0.8))
        ax3.set_title('Redshift Dependence\n(No Data Available)')
    
    # Plot 4: Uncertainty Quantification Analysis (if UQ data available)
    ax4 = axes[1, 0]
    if 'max_probability' in df.columns and not df['max_probability'].isna().all():
        uq_data = df.dropna(subset=['max_probability'])
        
        # Probability vs Distance Error
        scatter = ax4.scatter(uq_data['max_probability'], uq_data['distance_error'],
                            alpha=0.6, s=30, c=correct_detections[uq_data.index],
                            cmap='RdYlGn', edgecolors='black', linewidth=0.5)
        
        # Add detection threshold line if available
        if 'detection_threshold' in df.columns:
            threshold = df['detection_threshold'].iloc[0]
            ax4.axvline(threshold, color='red', linestyle='--', alpha=0.8, 
                       label=f'Detection Threshold: {threshold:.2f}')
            ax4.legend()
        
        ax4.set_xlabel('Maximum Probability', fontsize=18)
        ax4.set_ylabel('Distance Error (pixels)', fontsize=18)
        ax4.set_title('Uncertainty Quantification:\nProbability vs Error', fontsize=18)
        ax4.tick_params(axis='both', labelsize=18)
        ax4.grid(True, alpha=0.3)
        
        # Add colorbar
        cbar = plt.colorbar(scatter, ax=ax4)
        cbar.set_label('Correct Detection', rotation=270, labelpad=15)
    else:
        ax4.text(0.5, 0.5, 'No uncertainty\nquantification data\navailable', ha='center', va='center',
                transform=ax4.transAxes, fontsize=12, bbox=dict(boxstyle="round,pad=0.3", 
                facecolor="lightgray", alpha=0.8))
        ax4.set_title('Uncertainty Quantification\n(No Data Available)')
    
    # Plot 5: Multi-Target Accuracy Comparison
    ax5 = axes[1, 1]
    if 'matches_any_target' in df.columns and 'n_targets' in df.columns:
        # Calculate multi-target accuracy
        multi_target_correct = df['matches_any_target'].sum()
        multi_target_accuracy = multi_target_correct / len(df) * 100

        # Calculate single-target accuracy for comparison
        single_target_correct = np.sum(correct_detections)
        single_target_accuracy = accuracy

        # Count images with multiple targets
        images_with_multiple_targets = len(df[df['n_targets'] > 1])
        pct_multiple_targets = images_with_multiple_targets / len(df) * 100

        # Create comparison bar chart
        metrics = ['Single-Target\nAccuracy', 'Multi-Target\nAccuracy']
        values = [single_target_accuracy, multi_target_accuracy]
        colors_bar = ['#3498db', '#2ecc71']

        bars = ax5.bar(metrics, values, color=colors_bar, alpha=0.8, edgecolor='black', linewidth=1.5)

        # Add value labels on bars
        for bar, value in zip(bars, values):
            height = bar.get_height()
            ax5.text(bar.get_x() + bar.get_width()/2., height + 1,
                    f'{value:.1f}%', ha='center', va='bottom', fontsize=16, fontweight='bold')

        ax5.set_ylabel('Accuracy (%)', fontsize=18)
        ax5.set_title(f'Accuracy Comparison (≤10px)\n{images_with_multiple_targets} images ({pct_multiple_targets:.1f}%) have multiple targets',
                     fontweight='bold', fontsize=16)
        ax5.set_ylim(0, 105)
        ax5.tick_params(axis='both', labelsize=18)
        ax5.grid(True, alpha=0.3, axis='y')

        # Add improvement annotation if there's a difference
        improvement = multi_target_accuracy - single_target_accuracy
        if abs(improvement) > 0.1:
            ax5.text(0.5, 0.5, f'Improvement:\n{improvement:+.1f}%',
                    transform=ax5.transAxes, ha='center', va='center',
                    fontsize=14, bbox=dict(boxstyle="round,pad=0.5",
                    facecolor='yellow' if improvement > 0 else 'lightcoral', alpha=0.3))
    else:
        ax5.text(0.5, 0.5, 'Multi-target matching\ndata not available\n(run test.py to generate)',
                ha='center', va='center', transform=ax5.transAxes, fontsize=12,
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.8))
        ax5.set_title('Multi-Target Accuracy\n(No Data Available)', fontsize=16)
    
    # Plot 6: Performance Summary Statistics
    ax6 = axes[1, 2]
    ax6.axis('off')  # Remove axes for text summary
    
    # Calculate comprehensive statistics
    stats_text = []
    stats_text.append("PERFORMANCE SUMMARY")
    stats_text.append("=" * 25)
    stats_text.append(f"Total Samples: {len(df)}")
    stats_text.append(f"Single-Target Acc: {accuracy:.1f}%")

    # Add multi-target accuracy if available
    if 'matches_any_target' in df.columns and 'n_targets' in df.columns:
        multi_target_correct = df['matches_any_target'].sum()
        multi_target_accuracy = multi_target_correct / len(df) * 100
        stats_text.append(f"Multi-Target Acc: {multi_target_accuracy:.1f}%")
        images_with_multiple = len(df[df['n_targets'] > 1])
        stats_text.append(f"Images w/ Multiple Targets: {images_with_multiple}")
    stats_text.append("")
    
    # Distance statistics
    stats_text.append("DISTANCE ERROR STATISTICS")
    stats_text.append("-" * 25)
    stats_text.append(f"Mean Error: {np.mean(distances):.2f} px")
    stats_text.append(f"Median Error: {np.median(distances):.2f} px")
    stats_text.append(f"Std Dev: {np.std(distances):.2f} px")
    stats_text.append(f"90th Percentile: {np.percentile(distances, 90):.2f} px")
    stats_text.append("")
    
    # UQ statistics if available
    if 'max_probability' in df.columns and not df['max_probability'].isna().all():
        uq_data = df.dropna(subset=['max_probability'])
        stats_text.append("UNCERTAINTY QUANTIFICATION")
        stats_text.append("-" * 25)
        stats_text.append(f"Mean Max Prob: {np.mean(uq_data['max_probability']):.3f}")
        stats_text.append(f"Median Max Prob: {np.median(uq_data['max_probability']):.3f}")
        
        if 'n_detections' in df.columns:
            mean_detections = np.mean(df['n_detections'].dropna())
            stats_text.append(f"Avg Detections/Image: {mean_detections:.1f}")
    
    # Redshift statistics if available
    if 'z' in df.columns and not df['z'].isna().all():
        z_data = df.dropna(subset=['z'])
        stats_text.append("")
        stats_text.append("REDSHIFT STATISTICS")
        stats_text.append("-" * 25)
        stats_text.append(f"Redshift Range: {z_data['z'].min():.3f} - {z_data['z'].max():.3f}")
        stats_text.append(f"Mean Redshift: {np.mean(z_data['z']):.3f}")
    
    # Display text
    ax6.text(0.05, 0.95, '\n'.join(stats_text), transform=ax6.transAxes, fontsize=18,
            verticalalignment='top', fontfamily='monospace',
            bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.3))
    
    # Adjust layout and save
    plt.tight_layout()
    
    # Save the plot
    output_file = os.path.join(output_dir, 'diagnostic_plots.png')
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"Diagnostic plots saved to: {output_file}")
    
    # Also save as PDF for publication quality
    pdf_file = os.path.join(output_dir, 'diagnostic_plots.pdf')
    plt.savefig(pdf_file, dpi=300, bbox_inches='tight')
    print(f"High-quality PDF saved to: {pdf_file}")
    
    return fig


def main():
    """Command line interface for diagnostic plotting."""
    import argparse
    
    parser = argparse.ArgumentParser(description='Generate diagnostic plots from BCG evaluation results')
    parser.add_argument('results_file', help='Path to evaluation_results.csv file')
    parser.add_argument('--output_dir', help='Output directory for plots (default: same as results file)')
    parser.add_argument('--figsize', nargs=2, type=float, default=[16, 12], 
                       help='Figure size (width height) in inches')
    
    args = parser.parse_args()
    
    try:
        fig = create_diagnostic_plots(args.results_file, args.output_dir, tuple(args.figsize))
        plt.show()
    except Exception as e:
        print(f"Error creating diagnostic plots: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()